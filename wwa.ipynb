{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake_accident_data.csv')\n",
    "# 数值型数据填充缺失值（例如使用均值）  \n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "df[['PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT']] = imputer.fit_transform(df[['PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT']])  \n",
    "  \n",
    "# 文本数据填充缺失值（例如使用空字符串）  \n",
    "df['LOCATION'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数值型数据的异常值处理（例如使用IQR方法）  \n",
    "Q1 = df['VEHCOUNT'].quantile(0.25)  \n",
    "Q3 = df['VEHCOUNT'].quantile(0.75)  \n",
    "IQR = Q3 - Q1  \n",
    "df = df[(df['VEHCOUNT'] >= Q1 - 1.5 * IQR) & (df['VEHCOUNT'] <= Q3 + 1.5 * IQR)]  \n",
    "  \n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\24879\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\24879\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  \n",
    "nltk.download('punkt')  # 用于分词  \n",
    "nltk.download('stopwords')  # 用于停用词\n",
    "# NLP处理函数（分词、去停用词）  \n",
    "def preprocess_text(text):  \n",
    "    tokens = word_tokenize(text)  \n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    filtered_text = [word for word in tokens if not word in stop_words]  \n",
    "    return ' '.join(filtered_text)  \n",
    "  \n",
    "# 应用NLP处理函数到文本列  \n",
    "df['LOCATION_PROCESSED'] = df['LOCATION'].apply(preprocess_text)  \n",
    "  \n",
    "# 词向量化处理（例如使用TF-IDF）  \n",
    "vectorizer = TfidfVectorizer()  \n",
    "location_tfidf = vectorizer.fit_transform(df['LOCATION_PROCESSED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储数值型数据和原始文本数据  \n",
    "df.to_csv('preprocessed_data.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "  \n",
    "# 加载预处理后的数据  \n",
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OBJECTID                int64\n",
       "REPORTNO               object\n",
       "STATUS                 object\n",
       "ADDRTYPE               object\n",
       "LOCATION               object\n",
       "COLLISIONTYPE          object\n",
       "PERSONCOUNT           float64\n",
       "PEDCOUNT              float64\n",
       "PEDCYLCOUNT           float64\n",
       "VEHCOUNT              float64\n",
       "INJURIES                int64\n",
       "SERIOUSINJURIES         int64\n",
       "FATALITIES              int64\n",
       "INCDATE                object\n",
       "INCDTTM                object\n",
       "WEATHER                object\n",
       "ROADCOND               object\n",
       "LIGHTCOND              object\n",
       "SPEEDING               object\n",
       "HITPARKEDCAR           object\n",
       "LOCATION_PROCESSED     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns  \n",
    "correlation_matrix = df[numerical_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 OBJECTID  PERSONCOUNT  PEDCOUNT  PEDCYLCOUNT  VEHCOUNT  \\\n",
      "OBJECTID         1.000000     0.000352  0.042831    -0.000717  0.011049   \n",
      "PERSONCOUNT      0.000352     1.000000  0.031991     0.041986 -0.012406   \n",
      "PEDCOUNT         0.042831     0.031991  1.000000     0.044405  0.008279   \n",
      "PEDCYLCOUNT     -0.000717     0.041986  0.044405     1.000000  0.046239   \n",
      "VEHCOUNT         0.011049    -0.012406  0.008279     0.046239  1.000000   \n",
      "INJURIES         0.057779     0.043021  0.033252    -0.014158  0.028863   \n",
      "SERIOUSINJURIES -0.035462     0.050300  0.022874    -0.056013  0.029299   \n",
      "FATALITIES      -0.021814     0.025310  0.031611    -0.037888  0.023138   \n",
      "\n",
      "                 INJURIES  SERIOUSINJURIES  FATALITIES  \n",
      "OBJECTID         0.057779        -0.035462   -0.021814  \n",
      "PERSONCOUNT      0.043021         0.050300    0.025310  \n",
      "PEDCOUNT         0.033252         0.022874    0.031611  \n",
      "PEDCYLCOUNT     -0.014158        -0.056013   -0.037888  \n",
      "VEHCOUNT         0.028863         0.029299    0.023138  \n",
      "INJURIES         1.000000         0.077650    0.034549  \n",
      "SERIOUSINJURIES  0.077650         1.000000    0.030046  \n",
      "FATALITIES       0.034549         0.030046    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 识别数值列  \n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns  \n",
    "  \n",
    "# 选择数值列并计算相关性矩阵  \n",
    "correlation_matrix = df[numerical_cols].corr()  \n",
    "  \n",
    "# 打印相关性矩阵  \n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OBJECTID', 'REPORTNO', 'STATUS', 'ADDRTYPE', 'LOCATION',\n",
      "       'COLLISIONTYPE', 'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT',\n",
      "       'INJURIES', 'SERIOUSINJURIES', 'FATALITIES', 'INCDATE', 'INCDTTM',\n",
      "       'WEATHER', 'ROADCOND', 'LIGHTCOND', 'SPEEDING', 'HITPARKEDCAR',\n",
      "       'LOCATION_PROCESSED'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "皮尔逊相关系数: -0.0124\n",
      "p值: 0.6952\n",
      "我们不能拒绝原假设,没有足够的证据表明PERSONCOUNT和VEHCOUNT之间存在显著关系。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from scipy import stats  \n",
    "  \n",
    "# 读取数据  \n",
    "data = pd.read_csv('preprocessed_data.csv')  \n",
    "  \n",
    "# 选择变量  \n",
    "person_count = data['PERSONCOUNT']  \n",
    "veh_count = data['VEHCOUNT']  \n",
    "  \n",
    "# 执行假设检验  \n",
    "correlation, p_value = stats.pearsonr(person_count, veh_count)  \n",
    "  \n",
    "# 打印结果  \n",
    "print(f\"皮尔逊相关系数: {correlation:.4f}\")  \n",
    "print(f\"p值: {p_value:.4f}\")  \n",
    "  \n",
    "# 解释结果  \n",
    "alpha = 0.05  # 显著性水平  \n",
    "if p_value < alpha:  \n",
    "    print(\"我们拒绝原假设,认为PERSONCOUNT和VEHCOUNT之间存在显著关系。\")  \n",
    "else:  \n",
    "    print(\"我们不能拒绝原假设,没有足够的证据表明PERSONCOUNT和VEHCOUNT之间存在显著关系。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "  \n",
    "# 加载数据  \n",
    "data = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'REPORT30'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22712\\726251695.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 创建随机森林分类器实例\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrf_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mrf_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1149\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \"\"\"\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    349\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         )\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    618\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         raise ValueError(\n\u001b[0;32m   1143\u001b[0m             \u001b[1;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m         )\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1147\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    912\u001b[0m                         )\n\u001b[0;32m    913\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m                 raise ValueError(\n\u001b[0;32m    918\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m         if (\n\u001b[0;32m   2086\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'REPORT30'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "  \n",
    "# 假设最后一列是目标变量，其余列是特征变量  \n",
    "X = data.iloc[:, :-1]  \n",
    "y = data.iloc[:, -1]  \n",
    "  \n",
    "# 划分训练集和测试集  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "  \n",
    "# 创建随机森林分类器实例  \n",
    "rf_classifier = RandomForestClassifier(random_state=42)  \n",
    "  \n",
    "# 训练模型  \n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import confusion_matrix, classification_report  \n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from mlxtend.preprocessing import TransactionEncoder  \n",
    "from mlxtend.frequent_patterns import apriori, association_rules  \n",
    "  \n",
    "# 1. 数据加载与预处理  \n",
    "data = pd.read_csv('yiyan_file_input.csv')  \n",
    "data['INCDATE'] = pd.to_datetime(data['INCDATE']).dt.date  \n",
    "data['INCDTTM'] = pd.to_datetime(data['INCDTTM'])  \n",
    "  \n",
    "# 假设我们需要对分类变量进行编码  \n",
    "le = LabelEncoder()  \n",
    "cat_cols = ['STATUS', 'ADDRTYPE', 'COLLISIONTYPE', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'SPEEDING', 'HITPARKEDCAR']  \n",
    "for col in cat_cols:  \n",
    "    data[col] = le.fit_transform(data[col])  \n",
    "  \n",
    "# 2. 数据分析与建模  \n",
    "# 选择特征和目标变量  \n",
    "X = data.drop('REPORTNO', axis=1)  # 假设我们用除了REPORTNO之外的所有字段来预测STATUS  \n",
    "y = data['STATUS']  \n",
    "X = X.drop('STATUS', axis=1)  \n",
    "  \n",
    "# 划分数据集  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "  \n",
    "# 构建随机森林模型  \n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "rf_clf.fit(X_train, y_train)  \n",
    "  \n",
    "# 交叉验证  \n",
    "scores = cross_val_score(rf_clf, X_train, y_train, cv=5)  \n",
    "print(f\"Cross-validation scores: {scores.mean()} ± {scores.std()}\")  \n",
    "  \n",
    "# 评估模型  \n",
    "y_pred = rf_clf.predict(X_test)  \n",
    "print(\"Confusion Matrix:\")  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(\"\\nClassification Report:\")  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "  \n",
    "# 3. 聚类分析  \n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  \n",
    "X_cluster = data.drop(['REPORTNO', 'STATUS', 'INCDATE', 'INCDTTM'], axis=1)  # 假设我们不使用这些字段进行聚类  \n",
    "kmeans.fit(X_cluster)  \n",
    "data['Cluster'] = kmeans.labels_  \n",
    "  \n",
    "# 4. 关联规则挖掘  \n",
    "# 为了关联规则挖掘，我们需要将数据转换为交易格式  \n",
    "te = TransactionEncoder()  \n",
    "te_ary = te.fit(data.drop(['OBJECTID', 'REPORTNO', 'INCDATE', 'INCDTTM'], axis=1).apply(lambda x: x.astype(str)).values.tolist())  \n",
    "df_te = pd.DataFrame(te_ary, columns=te.columns_)  \n",
    "  \n",
    "# 使用Apriori算法找出频繁项集  \n",
    "frequent_itemsets = apriori(df_te, min_support=0.05, use_colnames=True)  \n",
    "  \n",
    "# 生成关联规则  \n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)  \n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机森林分类器建模与评估  \n",
    "# 划分训练集和测试集  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "# 创建随机森林分类器  \n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "# 交叉验证评估模型性能  \n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)  \n",
    "print(f\"Cross-validation scores: {scores.mean()} ± {scores.std()}\")  \n",
    "# 训练模型  \n",
    "clf.fit(X_train, y_train)  \n",
    "# 预测测试集  \n",
    "y_pred = clf.predict(X_test)  \n",
    "# 输出混淆矩阵和分类报告  \n",
    "print(\"Confusion Matrix:\")  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(\"\\nClassification Report:\")  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K均值聚类分析  \n",
    "# 选择部分特征进行聚类  \n",
    "clustering_data = data[['PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT']]  \n",
    "# K均值聚类  \n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  \n",
    "kmeans.fit(clustering_data)  \n",
    "# 添加聚类标签到原始数据  \n",
    "data['Cluster'] = kmeans.labels_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori关联规则挖掘  \n",
    "# 选择需要进行关联规则挖掘的特征，这里以是否超速和是否撞到停放的车辆为例  \n",
    "transactions = data[['SPEEDING', 'HITPARKEDCAR']].applymap(lambda x: 'Y' if x == 'Y' else None).dropna(how='all')  \n",
    "# 将数据转换为适合Apriori算法的格式  \n",
    "te = TransactionEncoder()  \n",
    "te_ary = te.fit(transactions).transform(transactions)  \n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)  \n",
    "# 使用Apriori算法找出频繁项集  \n",
    "frequent_itemsets = apriori(df, min_support=0.07, use_colnames=True)  \n",
    "# 生成关联规则  \n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)  \n",
    "# 输出关联规则  \n",
    "print(\"Association Rules:\")  \n",
    "print(rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
