{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['https://www.stats.gov.cn/zt_18555/ztsj/sjzsj/sjz2007/202303/t20230303_1925009.html']\n",
    "    def parse(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        traffic_accidents = soup.find_all('div', class_='traffic_accident')\n",
    "        for accident in traffic_accidents:\n",
    "            pass\n",
    "def fetch_data_from_api():\n",
    "    response = requests.get('https://www.stats.gov.cn/zt_18555/ztsj/sjzsj/sjz2007/202303/t20230303_1925009.html')\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "def import_data_from_file(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "    return data\n",
    "\n",
    "def store_data_to_database(data):\n",
    "    conn = sqlite3.connect('traffic_accidents.db')\n",
    "    data.to_sql('traffic_accidents', conn, if_exists='replace')\n",
    "    conn.close()\n",
    "    \n",
    "process = scrapy.crawler.CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()\n",
    "\n",
    "api_data = fetch_data_from_api()\n",
    "file_data = import_data_from_file('traffic_accidents.csv')\n",
    "merged_data = pd.concat([api_data, file_data])\n",
    "\n",
    "store_data_to_database(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "\n",
    "    # 定义爬取的起始网址\n",
    "    start_urls = ['https://www.stats.gov.cn/zt_18555/ztsj/sjzsj/sjz2007/202303/t20230303_1925009.html']\n",
    "\n",
    "    # 定义爬取网页内容的处理函数\n",
    "    def parse(self, response):\n",
    "        # 使用BeautifulSoup解析网页内容\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # 提取交通事故数据\n",
    "        traffic_accidents = soup.find_all('div', class_='traffic_accident')\n",
    "        for accident in traffic_accidents:\n",
    "            # 解析数据并存储到数据库中\n",
    "            pass\n",
    "\n",
    "def fetch_data_from_api():\n",
    "    # 使用API接口获取数据\n",
    "    response = requests.get('http://example.com/api/traffic_accidents')\n",
    "    data = response.json()\n",
    "    # 返回结构化数据\n",
    "    return data\n",
    "def import_data_from_file(filename):\n",
    "    # 从本地文件导入数据\n",
    "    data = pd.read_csv(filename)\n",
    "    # 返回数据\n",
    "    return data\n",
    "def store_data_to_database(data):\n",
    "    # 连接数据库\n",
    "    conn = sqlite3.connect('traffic_accidents.db')\n",
    "    # 将数据存储到数据库中\n",
    "    data.to_sql('traffic_accidents', conn, if_exists='replace')\n",
    "    # 关闭数据库连接\n",
    "    conn.close()\n",
    "\n",
    "# 爬取网页数据\n",
    "process = scrapy.crawler.CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()\n",
    "# 获取API数据\n",
    "api_data = fetch_data_from_api()\n",
    "# 导入本地文件数据\n",
    "file_data = import_data_from_file('traffic_accidents.csv')\n",
    "# 合并数据\n",
    "merged_data = pd.concat([api_data, file_data])\n",
    "# 存储数据到数据库\n",
    "store_data_to_database(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sqlite3\n",
    "\n",
    "# 加载停用词列表\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# 读取原始数据\n",
    "raw_data = pd.read_csv('traffic_accidents.csv')\n",
    "# 缺失值填充\n",
    "processed_data = raw_data.fillna(method='ffill')\n",
    "# 异常值处理\n",
    "processed_data = processed_data[(processed_data['speed'] >= 0) & (processed_data['speed'] <= 120)]\n",
    "# 删除重复值\n",
    "processed_data = processed_data.drop_duplicates()\n",
    "# 文本数据处理\n",
    "def text_processing(text):\n",
    "    tokens = word_tokenize(text)  # 分词\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # 去停用词\n",
    "    return ' '.join(filtered_tokens)\n",
    "processed_data['text'] = processed_data['text'].apply(text_processing)\n",
    "\n",
    "# 使用TF-IDF进行文本数据向量化处理\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "text_vectors = tfidf_vectorizer.fit_transform(processed_data['text'])\n",
    "# 将文本向量化后的数据转换为DataFrame\n",
    "text_df = pd.DataFrame(text_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "# 合并文本向量化后的数据与原始数据\n",
    "processed_data = pd.concat([processed_data, text_df], axis=1)\n",
    "# 存储预处理后的数据到数据仓库中\n",
    "conn = sqlite3.connect('traffic_accidents.db')\n",
    "processed_data.to_sql('processed_data', conn, if_exists='replace')\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 读取预处理后的数据\n",
    "processed_data = pd.read_csv('processed_data.csv')\n",
    "# 描述性统计\n",
    "descriptive_stats = processed_data.describe()\n",
    "# 相关性分析\n",
    "correlation_matrix = processed_data.corr()\n",
    "# 假设检验\n",
    "t_statistic, p_value = stats.ttest_ind(processed_data['feature1'], processed_data['feature2'])\n",
    "# 构建预测模型\n",
    "X = processed_data.drop(['target'], axis=1)\n",
    "y = processed_data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# 交叉验证评估模型\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "# 输出交叉验证评估结果\n",
    "print(\"交叉验证评估结果：\", cv_scores)\n",
    "print(\"平均准确率：\", np.mean(cv_scores))\n",
    "\n",
    "# 输出混淆矩阵和分类报告\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"混淆矩阵：\")\n",
    "print(conf_matrix)\n",
    "print(\"分类报告：\")\n",
    "print(class_report)\n",
    "# 数据挖掘算法应用（聚类分析）\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "processed_data['cluster'] = kmeans.fit_predict(X)\n",
    "# 数据挖掘算法应用（关联规则挖掘）\n",
    "frequent_itemsets = apriori(processed_data, min_support=0.3, use_colnames=True)\n",
    "association_rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)\n",
    "# 输出关联规则挖掘结果\n",
    "print(\"关联规则挖掘结果：\")\n",
    "print(association_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# 读取预处理后的数据\n",
    "processed_data = pd.read_csv('processed_data.csv')\n",
    "# 描述性统计\n",
    "descriptive_stats = processed_data.describe()\n",
    "# 相关性分析\n",
    "correlation_matrix = processed_data.corr()\n",
    "# 构建预测模型\n",
    "X = processed_data.drop(['target'], axis=1)\n",
    "y = processed_data['target']\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# 绘制散点图\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis')\n",
    "plt.title('PCA Scatter Plot')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# 可视化界面\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1('交通事故数据分析系统'),\n",
    "    html.Div([\n",
    "        dcc.Graph(id='scatter-plot')\n",
    "    ]),\n",
    "    html.Div([\n",
    "        html.Label('选择特征进行PCA降维:'),\n",
    "        dcc.Dropdown(\n",
    "            id='pca-dropdown',\n",
    "            options=[{'label': col, 'value': col} for col in X.columns],\n",
    "            value=X.columns[0]\n",
    "        )\n",
    "    ])\n",
    "])\n",
    "@app.callback(\n",
    "    Output('scatter-plot', 'figure'),\n",
    "    [Input('pca-dropdown', 'value')]\n",
    ")\n",
    "def update_scatter_plot(selected_feature):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X[[selected_feature, 'feature2']])\n",
    "    scatter_plot = px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=y)\n",
    "    scatter_plot.update_layout(title='PCA Scatter Plot', xaxis_title='Principal Component 1',\n",
    "                                yaxis_title='Principal Component 2')\n",
    "    return scatter_plot\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/process_data', methods=['POST'])\n",
    "def process_data():\n",
    "    # 获取前端传来的数据\n",
    "    data = request.get_json()\n",
    "    # 进行数据处理和分析（模拟）\n",
    "    processed_result = {'result': 'Processed data'}\n",
    "    # 返回处理结果\n",
    "    return jsonify(processed_result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>交通事故数据分析系统</title>\n",
    "    <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\">\n",
    "    <style>\n",
    "        /* 自定义样式 */\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1 class=\"mt-5\">交通事故数据分析系统</h1>\n",
    "        <div class=\"row mt-5\">\n",
    "            <div class=\"col-md-6\">\n",
    "                <h3>数据处理</h3>\n",
    "                <form id=\"data-form\">\n",
    "                    <div class=\"form-group\">\n",
    "                        <label for=\"data-input\">输入数据:</label>\n",
    "                        <input type=\"text\" class=\"form-control\" id=\"data-input\">\n",
    "                    </div>\n",
    "                    <button type=\"submit\" class=\"btn btn-primary\">处理数据</button>\n",
    "                </form>\n",
    "            </div>\n",
    "            <div class=\"col-md-6\">\n",
    "                <h3>处理结果</h3>\n",
    "                <div id=\"result\"></div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script src=\"https://code.jquery.com/jquery-3.5.1.min.js\"></script>\n",
    "    <script>\n",
    "        $(document).ready(function() {\n",
    "            $('#data-form').submit(function(e) {\n",
    "                e.preventDefault();\n",
    "\n",
    "                // 获取输入的数据\n",
    "                var inputData = $('#data-input').val();\n",
    "\n",
    "                // 发送数据给后端进行处理\n",
    "                $.ajax({\n",
    "                    type: 'POST',\n",
    "                    url: '/process_data',\n",
    "                    contentType: 'application/json',\n",
    "                    data: JSON.stringify({ 'data': inputData }),\n",
    "                    success: function(response) {\n",
    "                        $('#result').html('<p>' + response.result + '</p>');\n",
    "                    },\n",
    "                    error: function() {\n",
    "                        $('#result').html('<p style=\"color: red;\">处理数据时出错。</p>');\n",
    "                    }\n",
    "                });\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
